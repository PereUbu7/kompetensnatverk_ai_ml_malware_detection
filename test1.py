import tensorflow as tf
from tensorflow.python.ops import variables
from tensorflow.python.framework import dtypes

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt


import encoder

filename = 'dataset.csv'

# Load csv data into panda dataframe
data = pd.read_csv(filename, sep=',', header=None)

# Replaces class labels with numbers
data = data.replace(encoder.classes_replace)

# Sample x number of rows from dataset
data = data.sample(10000)

# Split into train and test sets
dataTrain, dataTest = train_test_split(data, test_size=0.2)

# Extract features
X_train = np.array(dataTrain)[:,1:]
X_test = np.array(dataTest)[:,1:]

# Extract labels
Y_train = np.array(dataTrain)[:,0]
Y_test = np.array(dataTest)[:,0]

# Config variables
dim = X_train.shape[1]
n_train_samples = X_train.shape[0]
n_test_samples = X_test.shape[0]
batch_size = 1000
n_of_experts = 5
classificationThresholds = [0.9]

# Make all data into tf supported shapes
X_data = np.reshape(X_train, (n_train_samples,dim))
X_test_data = np.reshape(X_test, (n_test_samples,dim))
y_data = np.reshape(Y_train, (n_train_samples,1))
Y_test_data = np.reshape(Y_test, (n_test_samples,1))

# Beginning of computation graph
# -----------------------------

# Placeholders for input data (features, lables and learning rate)
X = tf.placeholder(tf.float32, shape=(batch_size, dim))
y = tf.placeholder(tf.float32, shape=(batch_size, 1))

learningRate = tf.placeholder(tf.float32)

manager_layer1 = tf.contrib.layers.fully_connected(
    X,
    200,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=tf.initializers.random_normal(stddev=0.1),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(1),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)

manager_layer2 = tf.contrib.layers.fully_connected(
    manager_layer1,
    50,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=tf.initializers.random_normal(stddev=0.1),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(1),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)


manager = tf.contrib.layers.fully_connected(
    manager_layer2,
    n_of_experts,
    activation_fn=tf.nn.softmax,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=tf.initializers.random_normal(stddev=0.1),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(1),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)

experts_layer1 = tf.contrib.layers.fully_connected(
    X,
    num_outputs=500,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=tf.initializers.random_normal(stddev=0.1),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(1),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)

experts_layer2 = tf.contrib.layers.fully_connected(
    experts_layer1,
    num_outputs=100,
    activation_fn=tf.nn.relu,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=tf.initializers.random_normal(stddev=0.1),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(1),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)

expertsOut = tf.contrib.layers.fully_connected(
    experts_layer2,
    num_outputs=n_of_experts,
    activation_fn=None,
    normalizer_fn=None,
    normalizer_params=None,
    weights_initializer=tf.initializers.random_normal(stddev=0.1),
    weights_regularizer=None,
    biases_initializer=tf.zeros_initializer(1),
    biases_regularizer=None,
    reuse=None,
    variables_collections=None,
    outputs_collections=None,
    trainable=True,
    scope=None
)

# Let manager select expert output
output = expertsOut*manager

# Softmax output
output_sum = tf.reduce_sum(expertsOut*manager, axis=1, keepdims=True)
outputClass = tf.nn.sigmoid(output_sum)

[true_positives, updateTruePositivesOp] = tf.metrics.true_positives_at_thresholds(y, outputClass, classificationThresholds)
[true_negatives, updateTrueNegativesOp] = tf.metrics.true_negatives_at_thresholds(y, outputClass, classificationThresholds)
[false_positives, updateFalsePositivesOp] = tf.metrics.false_positives_at_thresholds(y, outputClass, classificationThresholds)
[false_negatives, updateFalseNegativesOp] = tf.metrics.false_negatives_at_thresholds(y, outputClass, classificationThresholds)

loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=outputClass, labels=y), axis=0)

global_step = variables.Variable(0, dtype=dtypes.int64)

opt_operation = tf.train.RMSPropOptimizer(learning_rate=learningRate).minimize(loss)

# End of computation graph
# ------------------------------


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    # Initialize true_positives etc...
    sess.run(tf.local_variables_initializer())

    # Init logging arrays
    i_array = []
    loss_train_array = []
    loss_test_array = []
    TrueP_array = []
    TrueN_array = []
    FalseP_array = []
    FalseN_array = []

    # Number of iteration steps
    n_of_steps = 4000

    # Create plot window
    fig, (ax1, ax2) = plt.subplots(1, 2)

    # Start training
    for i in range(n_of_steps):

        # Pick training samples
        indices = np.random.choice(n_train_samples, batch_size)
        X_batch, y_batch = X_data[indices], y_data[indices]

        # Train on training samples
        _, loss_val = sess.run([opt_operation, loss], feed_dict={X: X_batch, y: y_batch, learningRate: 0.01})

        # Save training loss log
        loss_train_array.append(loss_val)

        # Pick test samples
        testIndices = np.random.choice(n_test_samples, batch_size)
        X_test_batch, Y_test_batch = X_test_data[testIndices], Y_test_data[testIndices]

        # Evalute ANN on test data
        # Get loss and sensitivity analysis
        # What's sensitivity? Check out here: https://en.wikipedia.org/wiki/Sensitivity_and_specificity
        testLoss, _,_,_,_, TrueP, TrueN, FalseP, FalseN = sess.run([loss, updateTruePositivesOp, updateTrueNegativesOp, updateFalsePositivesOp, updateFalseNegativesOp, true_positives, true_negatives, false_positives, false_negatives], feed_dict={X: X_test_batch, y: Y_test_batch})

        # Save to log
        TrueP_array.append(TrueP/(i+1))
        TrueN_array.append(TrueN/(i+1))
        FalseP_array.append(FalseP/(i+1))
        FalseN_array.append(FalseN/(i+1))

        loss_test_array.append(testLoss)

        i_array.append(i)

        # Print to plot window
        if i % 100 == 0:
            print("Step:", i, "of", n_of_steps)

            ax1.plot(i_array, loss_train_array, 'r', label="Trainingset loss")
            ax1.plot(i_array, loss_test_array, 'b', label="Testset loss")

            ax2.plot(i_array, TrueP_array, 'r', label="True Positives")
            ax2.plot(i_array, TrueN_array, 'b', label="True Negatives")
            ax2.plot(i_array, FalseP_array, 'g', label="False Positives")
            ax2.plot(i_array, FalseN_array, 'y', label="False Negatives")

            if i == 0:
                ax1.legend()
                ax2.legend()

            plt.show(block=False)
            plt.pause(0.001)

            i_array.clear()
            loss_train_array.clear()
            loss_test_array.clear()
            TrueP_array.clear()
            TrueN_array.clear()
            FalseP_array.clear()
            FalseN_array.clear()

    plt.show()
